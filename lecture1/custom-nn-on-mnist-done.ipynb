{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d25fcf1",
   "metadata": {},
   "source": [
    "## В данном ноутбуке будет описан полный цикл тренировки нейронной сети на numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a6506",
   "metadata": {},
   "source": [
    "Шаг 1. Сначала скачаем данные для тренировки теста и валидации. Это данные с рукописными цифрами - MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86886297",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/35c067adcc1ab364c8803830cdb34d0d50eea37e/week01_backprop/util.py -O util.py\n",
    "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/35c067adcc1ab364c8803830cdb34d0d50eea37e/week01_backprop/mnist.py -O mnist.py\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524f14e",
   "metadata": {},
   "source": [
    "Шаг 2. Опишем общий шаблон класса для каждого слоя нейросети. У каждого слоя нашей сетки будет всего 3 очень важных метода - это:\n",
    "\n",
    "1. Метод init для инициализации параметров и весов сети\n",
    "2. Метод forward, который принимает вход и выдает выход из нейросети\n",
    "3. Метод backward который принимает на вход два параметра: вход в текущий слой нейросети и градиент функции потерь по выходу для того, чтобы прокинуть градиент от выхода на вход."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c68f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Это абстрактный класс для блока нейросети. Каждый слой должен уметь делать две вещи:\n",
    "    \n",
    "    - Превращать входные данные в выходные:          output = layer.forward(input)\n",
    "    \n",
    "    - Прокидывать градиент через слой в обратную сторону:             grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    \n",
    "    Также некоторые слои будут иметь тренируемы параметры, которые должны обновляться в методе backward\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Инициализация параметрами\"\"\"\n",
    "        # В самом простом случае инициализировать нечего\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Берет входные данные размера [batch, input_units] и возвращает [batch, output_units] \n",
    "        \"\"\"\n",
    "        # В самом простом случае просто возвращаем вход\n",
    "        return input\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Пробрасывает градиент через слой.\n",
    "        \n",
    "        Для того, чтобы посчитать градиент по входу нейобходимо применить алгоритм обратного распространения ошибки:\n",
    "        \n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "        \n",
    "        В данной формуле уже известно, что grad_output = (d loss / d layer) и необходимо посчитать только градиенты \n",
    "        текущего слоя по входу - (d layer / d x) и перемножить\n",
    "        \n",
    "        Если у слоя есть параметры, то необходимо их обновить с помощью d loss / d layer\n",
    "        \"\"\"\n",
    "        # в простейше случае просто возвращаем grad_output и ничего не обновляем, но напишем это немного по-другому\n",
    "        num_units = input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        \n",
    "        return grad_output @ d_layer_d_input # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69081cb9",
   "metadata": {},
   "source": [
    "Что будет дальше?\n",
    "\n",
    "Далее напишем несколько блоков для нашей сетки:\n",
    "\n",
    "1. Линейный полносвязный слой - $f(X)=X \\cdot W + \\vec{b}$\n",
    "2. Слой активации ReLU\n",
    "3. Функцию потерь crossentropy\n",
    "4. Алгоритм обратного распространения ошибки - стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"Слой ReLU для нелинейности\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Необходимо применить к каждому элементу input - [batch, input_units]\"\"\"\n",
    "        output = np.maximum(input, 0)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
    "        relu_grad = input > 0\n",
    "        return grad_output * relu_grad        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8423826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some tests\n",
    "from util import eval_numerical_gradient\n",
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = ReLU()\n",
    "grads = l.backward(x, np.ones([10,32])/(32*10))\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=x)\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0),\\\n",
    "    \"gradient returned by your layer does not match the numerically computed gradient\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefdea02",
   "metadata": {},
   "source": [
    "Теперь сделаем что-нибудь немного более сложеное - линейные слой. В отличии от ReLU - линейный слой имеет обучаемые параметры внутри. Напомню форумулу: $f(X)=X \\cdot W + \\vec{b}$, где:\n",
    "\n",
    "1. X - [batch, num_inputs]\n",
    "2. W - [num_inputs, num_outputs]\n",
    "3. b - вектор размерности num_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        f(x) = <x*W> + b\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Инициализируем W небольшими числами из нормального распределения\n",
    "        # Есть лучшие инициализации весов сеток - за ними сюда: http://bit.ly/2vTlmaJ\n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        return input @ self.weights + self.biases\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        \"\"\"\n",
    "        Тут нужно продеферинцировать выход по входу для того, чтоб это вернуть на предыдущий слой\n",
    "        Также нужно продеферинцировать выход по весам и биасам для того, чтобы сделать град спуск по обучаемым параметрам\n",
    "        \"\"\"\n",
    "        # compute d f / d x = d f / d dense * d dense / d x\n",
    "        # where d dense/ d x = weights transposed\n",
    "        grad_input = grad_output @ self.weights.T\n",
    "        \n",
    "        # compute gradient w.r.t. weights and biases\n",
    "        grad_weights = input.T @ grad_output\n",
    "        #grad_biases = (np.ones(input.shape[0]).reshape(1, input.shape[0]) @ grad_output).ravel()\n",
    "        grad_biases = np.sum(grad_output, axis=0)\n",
    "        \n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        # Here we perform a stochastic gradient descent step. \n",
    "        # Later on, you can try replacing that with something better.\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1db22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Linear(128, 150)\n",
    "\n",
    "assert -0.05 < l.weights.mean() < 0.05 and 1e-3 < l.weights.std() < 1e-1,\\\n",
    "    \"The initial weights must have zero mean and small variance. \"\\\n",
    "    \"If you know what you're doing, remove this assertion.\"\n",
    "assert -0.05 < l.biases.mean() < 0.05, \"Biases must be zero mean. Ignore if you have a reason to do otherwise.\"\n",
    "\n",
    "# To test the outputs, we explicitly set weights with fixed values. DO NOT DO THAT IN ACTUAL NETWORK!\n",
    "l = Linear(3,4)\n",
    "\n",
    "x = np.linspace(-1,1,2*3).reshape([2,3])\n",
    "l.weights = np.linspace(-1,1,3*4).reshape([3,4])\n",
    "l.biases = np.linspace(-1,1,4)\n",
    "\n",
    "assert np.allclose(l.forward(x),np.array([[ 0.07272727,  0.41212121,  0.75151515,  1.09090909],\n",
    "                                          [-0.90909091,  0.08484848,  1.07878788,  2.07272727]]))\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test the grads, we use gradients obtained via finite differences\n",
    "\n",
    "from util import eval_numerical_gradient\n",
    "\n",
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = Linear(32,64,learning_rate=0)\n",
    "\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(),x)\n",
    "grads = l.backward(x,np.ones([10,64]))\n",
    "\n",
    "assert np.allclose(grads,numeric_grads,rtol=1e-3,atol=0), \"input gradient does not match numeric grad\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76bc25c",
   "metadata": {},
   "source": [
    "### The loss function\n",
    "\n",
    "Поскольку мы хотим предсказывать вероятности было бы логично сделать softmax, а потом уже функцию потерь. В данном случае будем реализовывать crossentropy loss.\n",
    "\n",
    "Обычный cross entropy loss: $$ loss = -\\sum y_i log \\hat{y_i}$$\n",
    "\n",
    "В данном случае получится:\n",
    "\n",
    "$$ loss = - log \\space {e^{a_{correct}} \\over {\\underset i \\sum e^{a_i} } } $$\n",
    "\n",
    "Лучше переписать вот так:\n",
    "\n",
    "$$ loss = - a_{correct} + log {\\underset i \\sum e^{a_i} } $$\n",
    "\n",
    "Последнее выражение лучше всего, поскольку:\n",
    "* Легче считать производные\n",
    "* Обычно быстрее считается"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df194cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits,target):\n",
    "    \"\"\"logits[batch,n_classes] target - [batch]. Подсчет лосса\"\"\"\n",
    "    logits_for_answers = logits[np.arange(len(logits)),target]\n",
    "    \n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    return xentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits,target):\n",
    "    \"\"\"logits[batch,n_classes] target - [batch]. Градиент по лоссу\"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)),target] = -1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e83c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.linspace(-1,1,500).reshape([50,10])\n",
    "answers = np.arange(50)%10\n",
    "\n",
    "softmax_crossentropy_with_logits(logits,answers)\n",
    "grads = grad_softmax_crossentropy_with_logits(logits,answers)\n",
    "numeric_grads = eval_numerical_gradient(lambda l: softmax_crossentropy_with_logits(l,answers).mean(),logits)\n",
    "\n",
    "assert np.allclose(numeric_grads,grads,rtol=1e-3,atol=0), \"The reference implementation has just failed. Someone has just changed the rules of math.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7e557",
   "metadata": {},
   "source": [
    "Шаг 3. Теперь давайте скомбинируем все слои вместе и построим нейронку для классификации рукописных цифр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ede4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "from mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f8754",
   "metadata": {},
   "source": [
    "Теперь определим нашу нейронную сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = []\n",
    "net.append(Linear(X_train.shape[1], 100))\n",
    "net.append(ReLU())\n",
    "net.append(Linear(100, 200))\n",
    "net.append(ReLU())\n",
    "net.append(Linear(200, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(net, X):\n",
    "    \"\"\"\n",
    "    Подсчет активаций для каждого слоя нейросети. Последняя активация - это логиты\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    input = X\n",
    "    \n",
    "    for layer in net:\n",
    "        activations.append(layer.forward(input))\n",
    "        input = activations[-1]\n",
    "        \n",
    "    assert len(activations) == len(net)\n",
    "\n",
    "    return activations\n",
    "\n",
    "def predict(net, X):\n",
    "    \"\"\"\n",
    "    Use network to predict the most likely class for each sample.\n",
    "    \"\"\"\n",
    "    logits = forward(net, X)[-1]\n",
    "    return logits.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eced1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,X,y):\n",
    "    \"\"\"\n",
    "    Train your network on a given batch of X and y.\n",
    "    You first need to run forward to get all layer activations.\n",
    "    You can estimate loss and loss_grad, obtaining dL / dy_pred\n",
    "    Then you can run layer.backward going from last layer to first, \n",
    "    propagating the gradient of input to previous layers.\n",
    "    \n",
    "    After you called backward for all layers, all Dense layers have already made one gradient step.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the layer activations\n",
    "    layer_activations = forward(net,X)\n",
    "    layer_inputs = [X] + layer_activations[:-1]  #layer_input[i] is an input for network[i]\n",
    "    logits = layer_activations[-1]\n",
    "    \n",
    "    # Compute the loss and the initial gradient\n",
    "    loss = softmax_crossentropy_with_logits(logits,y)\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)    \n",
    "    # propagate gradients through network layers using .backward\n",
    "    # hint: start from last layer and move to earlier layers\n",
    "    grad_output = loss_grad\n",
    "    for layer, layer_input in zip(net[::-1], layer_inputs[::-1]):\n",
    "        grad_output = layer.backward(layer_input, grad_output)\n",
    "        \n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03115fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e77b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
    "        train(net, x_batch, y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(predict(net, X_train) == y_train))\n",
    "    val_log.append(np.mean(predict(net, X_val) == y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch + 1)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b13922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
