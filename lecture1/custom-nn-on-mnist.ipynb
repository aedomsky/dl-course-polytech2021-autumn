{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59986411",
   "metadata": {},
   "source": [
    "## В данном ноутбуке будет описан полный цикл тренировки нейронной сети на numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d83287",
   "metadata": {},
   "source": [
    "Шаг 1. Сначала скачаем данные для тренировки теста и валидации. Это данные с рукописными цифрами - MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/35c067adcc1ab364c8803830cdb34d0d50eea37e/week01_backprop/util.py -O util.py\n",
    "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/35c067adcc1ab364c8803830cdb34d0d50eea37e/week01_backprop/mnist.py -O mnist.py\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264ad33",
   "metadata": {},
   "source": [
    "Шаг 2. Опишем общий шаблон класса для каждого слоя нейросети. У каждого слоя нашей сетки будет всего 3 очень важных метода - это:\n",
    "\n",
    "1. Метод init для инициализации параметров и весов сети\n",
    "2. Метод forward, который принимает вход и выдает выход из нейросети\n",
    "3. Метод backward который принимает на вход два параметра: вход в текущий слой нейросети и градиент функции потерь по выходу для того, чтобы прокинуть градиент от выхода на вход."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ad56cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Это абстрактный класс для блока нейросети. Каждый слой должен уметь делать две вещи:\n",
    "    \n",
    "    - Превращать входные данные в выходные:          output = layer.forward(input)\n",
    "    \n",
    "    - Прокидывать градиент через слой в обратную сторону:             grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    \n",
    "    Также некоторые слои будут иметь тренируемы параметры, которые должны обновляться в методе backward\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Инициализация параметрами\"\"\"\n",
    "        # В самом простом случае инициализировать нечего\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Берет входные данные размера [batch, input_units] и возвращает [batch, output_units] \n",
    "        \"\"\"\n",
    "        # В самом простом случае просто возвращаем вход\n",
    "        return input\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Пробрасывает градиент через слой.\n",
    "        \n",
    "        Для того, чтобы посчитать градиент по входу нейобходимо применить алгоритм обратного распространения ошибки:\n",
    "        \n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "        \n",
    "        В данной формуле уже известно, что grad_output = (d loss / d layer) и необходимо посчитать только градиенты \n",
    "        текущего слоя по входу - (d layer / d x) и перемножить\n",
    "        \n",
    "        Если у слоя есть параметры, то необходимо их обновить с помощью d loss / d layer\n",
    "        \"\"\"\n",
    "        # в простейше случае просто возвращаем grad_output и ничего не обновляем, но напишем это немного по-другому\n",
    "        num_units = input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input) # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4befb78e",
   "metadata": {},
   "source": [
    "Что будет дальше?\n",
    "\n",
    "Далее напишем несколько блоков для нашей сетки:\n",
    "\n",
    "1. Линейный полносвязный слой - $f(X)=X \\cdot W + \\vec{b}$\n",
    "2. Слой активации ReLU\n",
    "3. Функцию потерь crossentropy\n",
    "4. Алгоритм обратного распространения ошибки - стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95002990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"Слой ReLU для нелинейности\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Необходимо применить к каждому элементу input - [batch, input_units]\"\"\"\n",
    "        # <your code. Try np.maximum>\n",
    "        return output\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
    "        # <your code>\n",
    "        # relu_grad = ...\n",
    "        return grad_output * relu_grad        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some tests\n",
    "from util import eval_numerical_gradient\n",
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = ReLU()\n",
    "grads = l.backward(x, np.ones([10,32])/(32*10))\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=x)\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0),\\\n",
    "    \"gradient returned by your layer does not match the numerically computed gradient\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df0c74f",
   "metadata": {},
   "source": [
    "Теперь сделаем что-нибудь немного более сложеное - линейные слой. В отличии от ReLU - линейный слой имеет обучаемые параметры внутри. Напомню форумулу: $f(X)=X \\cdot W + \\vec{b}$, где:\n",
    "\n",
    "1. X - [batch, num_inputs]\n",
    "2. W - [num_inputs, num_outputs]\n",
    "3. b - вектор размерности num_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7263fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        f(x) = <x*W> + b\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Инициализируем W небольшими числами из нормального распределения\n",
    "        # Есть лучшие инициализации весов сеток - за ними сюда: http://bit.ly/2vTlmaJ\n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        return #<your code here>\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        \"\"\"\n",
    "        Тут нужно продеферинцировать выход по входу для того, чтоб это вернуть на предыдущий слой\n",
    "        Также нужно продеферинцировать выход по весам и биасам для того, чтобы сделать град спуск по обучаемым параметрам\n",
    "        \"\"\"\n",
    "        # вычислить d f / d x = d f / d dense * d dense / d x\n",
    "        grad_input = #<your code here>\n",
    "        \n",
    "        # вычислить градиенты по параметрам\n",
    "        grad_weights = #<your code here>\n",
    "        grad_biases = #<your code here>\n",
    "        \n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        # Here we perform a stochastic gradient descent step. \n",
    "        # Later on, you can try replacing that with something better.\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de9e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Linear(128, 150)\n",
    "\n",
    "assert -0.05 < l.weights.mean() < 0.05 and 1e-3 < l.weights.std() < 1e-1,\\\n",
    "    \"The initial weights must have zero mean and small variance. \"\\\n",
    "    \"If you know what you're doing, remove this assertion.\"\n",
    "assert -0.05 < l.biases.mean() < 0.05, \"Biases must be zero mean. Ignore if you have a reason to do otherwise.\"\n",
    "\n",
    "# To test the outputs, we explicitly set weights with fixed values. DO NOT DO THAT IN ACTUAL NETWORK!\n",
    "l = Linear(3,4)\n",
    "\n",
    "x = np.linspace(-1,1,2*3).reshape([2,3])\n",
    "l.weights = np.linspace(-1,1,3*4).reshape([3,4])\n",
    "l.biases = np.linspace(-1,1,4)\n",
    "\n",
    "assert np.allclose(l.forward(x),np.array([[ 0.07272727,  0.41212121,  0.75151515,  1.09090909],\n",
    "                                          [-0.90909091,  0.08484848,  1.07878788,  2.07272727]]))\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b19a81",
   "metadata": {},
   "source": [
    "### The loss function\n",
    "\n",
    "Поскольку мы хотим предсказывать вероятности было бы логично сделать softmax, а потом уже функцию потерь. В данном случае будем реализовывать crossentropy loss.\n",
    "\n",
    "Обычный cross entropy loss: $$ loss = -\\sum y_i log \\hat{y_i}$$\n",
    "\n",
    "В данном случае получится:\n",
    "\n",
    "$$ loss = - log \\space {e^{a_{correct}} \\over {\\underset i \\sum e^{a_i} } } $$\n",
    "\n",
    "Лучше переписать вот так:\n",
    "\n",
    "$$ loss = - a_{correct} + log {\\underset i \\sum e^{a_i} } $$\n",
    "\n",
    "Последнее выражение лучше всего, поскольку:\n",
    "* Легче считать производные\n",
    "* Обычно быстрее считается"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5adcdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits,target):\n",
    "    \"\"\"logits[batch,n_classes] target - [batch]. Подсчет лосса\"\"\"\n",
    "    logits_for_answers = logits[np.arange(len(logits)),target]\n",
    "    \n",
    "    xentropy = #<your code here>\n",
    "    \n",
    "    return xentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits,target):\n",
    "    \"\"\"logits[batch,n_classes] target - [batch]. Градиент по лоссу\"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    \n",
    "    #<your code here>\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da882197",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.linspace(-1,1,500).reshape([50,10])\n",
    "answers = np.arange(50)%10\n",
    "\n",
    "softmax_crossentropy_with_logits(logits,answers)\n",
    "grads = grad_softmax_crossentropy_with_logits(logits,answers)\n",
    "numeric_grads = eval_numerical_gradient(lambda l: softmax_crossentropy_with_logits(l,answers).mean(),logits)\n",
    "\n",
    "assert np.allclose(numeric_grads,grads,rtol=1e-3,atol=0), \"The reference implementation has just failed. Someone has just changed the rules of math.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db0a272",
   "metadata": {},
   "source": [
    "Шаг 3. Теперь давайте скомбинируем все слои вместе и построим нейронку для классификации рукописных цифр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d732735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "from mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76456914",
   "metadata": {},
   "source": [
    "Теперь определим нашу нейронную сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e066ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = []\n",
    "net.append(Linear(X_train.shape[1], 100))\n",
    "net.append(ReLU())\n",
    "net.append(Linear(100, 200))\n",
    "net.append(ReLU())\n",
    "net.append(Linear(200, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e10a52",
   "metadata": {},
   "source": [
    "Напишем foward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(net, X):\n",
    "    \"\"\"\n",
    "    Подсчет активаций для каждого слоя нейросети. Последняя активация - это логиты\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    input = X\n",
    "    \n",
    "    #<your code here>\n",
    "        \n",
    "    assert len(activations) == len(net)\n",
    "\n",
    "    return activations\n",
    "\n",
    "def predict(net, X):\n",
    "    \"\"\"\n",
    "    предсказание меток\n",
    "    \"\"\"\n",
    "    #<your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5545b",
   "metadata": {},
   "source": [
    "Напишем тренировку с backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef286038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,X,y):\n",
    "    # Активации слоев\n",
    "    layer_activations = forward(net,X)\n",
    "    layer_inputs = [X] + layer_activations[:-1]\n",
    "    logits = layer_activations[-1]\n",
    "    \n",
    "    # Вычисляем лосс и его градиент\n",
    "    loss = softmax_crossentropy_with_logits(logits,y)\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)    \n",
    "    # алгоритм backprop\n",
    "    \n",
    "    #<your code here>\n",
    "    \n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c838e00",
   "metadata": {},
   "source": [
    "Теперь все готово! Можно запускать..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b840ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
    "        train(net, x_batch, y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(predict(net, X_train) == y_train))\n",
    "    val_log.append(np.mean(predict(net, X_val) == y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch + 1)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
